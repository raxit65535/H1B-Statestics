{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is Coding Challange for InsightGlobal Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary Libraries for processing the CSV files\n",
    "import csv as csv\n",
    "import math\n",
    "import re as re\n",
    "import sys as sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Analysis of data set at any given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-3f745a09f0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# opening the .csv file from input folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/.csv'"
     ]
    }
   ],
   "source": [
    "# total time complexity for processing the entire dataset is O(n + nlogn + nlogn) - O(nlogn)\n",
    "#specifying two dictionaries in python for saving key:value pair of topten occupation and top tep states for \n",
    "# certified visa applications\n",
    "\n",
    "# dictionary to store key: SOC_CODE, and incremental value\n",
    "topten_occupation = dict()\n",
    "\n",
    "# dictionary to store key: EMPLOYER_STATE and incremental value\n",
    "topten_states = dict()\n",
    "\n",
    "# dictionary to map SOC_CODE to JOB_TITLE. \n",
    "# note: in the dataset, single SOC_CODE has multiple JOB_TITLE, I tried to run the dataset of year 2018, \n",
    "# for which we have the analysis available. Using SOC_CODE, I got almost same results as 2018 reports. \n",
    "# I am not sure how the JOB_TITLE is matched with SOC_CODE, here in my implementation, I took the first JOB_TITLE\n",
    "# occurance in dataset, which is associated with SOC_CODE.\n",
    "soc_jobtitle = dict()\n",
    "\n",
    "# variable for saving the index of CASE_SATUS attribute in tokens (or in row)\n",
    "case_status = int()\n",
    "\n",
    "# for saving index of SOC_CODE\n",
    "soc_code = int()\n",
    "\n",
    "# for saving index of JOB_TITLE\n",
    "job_title = int()\n",
    "\n",
    "# for saving index of EMPLOYER_STATE\n",
    "state = int()\n",
    "    \n",
    "# giving the filename using commnad line argument\n",
    "filename = sys.argv[-1]\n",
    "\n",
    "# opening the .csv file from input folder\n",
    "with open(filename) as csv_file:\n",
    "        \n",
    "  \n",
    "    # saving the tokenized attributes from the \"first line\" of csv into a variable\n",
    "    attributes = csv_file.readline().split(';')\n",
    "\n",
    "    # creating regular expression for identifying the required attribute from the dataset\n",
    "    regexp_certified = re.compile(r'STATUS')\n",
    "    regexp_soc = re.compile(r'SOC_CODE')\n",
    "    regexp_job_title = re.compile(r'JOB_TITLE')\n",
    "    regexp_state = re.compile(r'EMPLOYER_STATE')\n",
    "    \n",
    "    # the loop iteration will get us the indexes of the attribute we need in the dataset at each row\n",
    "    for i,j in enumerate(attributes,0):\n",
    "        \n",
    "        # printing the attributes just to get insights of data\n",
    "        print(i,j)\n",
    "        \n",
    "        if(regexp_certified.search(attributes[i])):\n",
    "            case_status = i\n",
    "        elif(regexp_soc.search(attributes[i])):\n",
    "            soc_code = i\n",
    "        elif(regexp_job_title.search(attributes[i])):\n",
    "            job_title = i\n",
    "        elif(regexp_state.search(attributes[i])):\n",
    "            state = i\n",
    "            \n",
    "    \n",
    "    # line count will keep the total count of number of certified case status into the dataset\n",
    "    line_count = 0\n",
    "    \n",
    "    # sequential processing of csv file using for loop - O (n)\n",
    "    for row in csv_file:\n",
    "        \n",
    "        # tokenizing the line by ';'\n",
    "        tokens = row.split(';')\n",
    "        \n",
    "        #key1 is for topten_occupation() dictionary & key2 is for topten_states() dictionary\n",
    "        key1 = tokens[soc_code]\n",
    "        key2 = tokens[state]\n",
    "\n",
    "        #processing only those cases which has status certified\n",
    "        if(tokens[case_status] == \"CERTIFIED\"):\n",
    "            \n",
    "            # logic to implement key:value pair in dict (search complexity is O(1))\n",
    "            if(key1 in topten_occupation):\n",
    "                topten_occupation[key1] += 1\n",
    "                \n",
    "            else:\n",
    "                # time complexity for insert is O(n)  \n",
    "                topten_occupation[key1] = 1\n",
    "                soc_jobtitle[key1] = tokens[job_title]\n",
    "                \n",
    "            if(key2 in topten_states):\n",
    "                topten_states[key2] += 1\n",
    "            else:\n",
    "                topten_states[key2] = 1\n",
    "            \n",
    "            # keeping track of number of certified cases    \n",
    "            line_count = line_count +1\n",
    "    \n",
    "    \n",
    "# sorting the dict - time complexity according to timesort algorithm - O(nlogn)    \n",
    "final_occupation = sorted(topten_occupation.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "final_states = sorted(topten_states.items(), key=lambda kv: kv[1], reverse=True)[:10]\n",
    "\n",
    "# just to get insight of total number of CERTIFIED cases, printing the counter\n",
    "print(\"\\n\\n Number of Lines in CSV file:\\t\", line_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Certified Occupations of year 2014: \n",
      "\n",
      "\n",
      "BUSINESS ANALYST 85109 18.699356687114406\n",
      "SENIOR APPLICATION DEVELOPER 68126 14.968010124268364\n",
      "PROGRAMMER ANALYST 64846 14.24735907756666\n",
      "IT QUALITY ASSURANCE AUTOMATION ANALYST 36207 7.955064770709929\n",
      "SOFTWARE ENGINEER 13796 3.031128609846554\n",
      "ASSOCIATE 10400 2.2849911236883274\n",
      "INTERNATIONAL ACCOUNTANT 8428 1.8517216529274252\n",
      "SENIOR INVESTMENT ANALYST 7687 1.6889160353646318\n",
      "SYSTEMS ENGINEER-2 (WEBSPHERE ADMINISTRATOR) 7276 1.598614943841949\n",
      "DIRECTOR OF ENGINEERING 6476 1.4228463958659237\n"
     ]
    }
   ],
   "source": [
    "print(\"Top-10 Certified Occupations of year 2014: \\n\\n\")\n",
    "\n",
    "# now have the all the data in the dictionaries, its time to export the o/p in .txt file\n",
    "with open(\"../output/Top_10_occupations.txt\",'w') as myoccupations:\n",
    "    for x,y in final_occupation:\n",
    "        # we had maintain the toal number of certified case count, (line_count), which we can use to calculate %\n",
    "        # latter the percentage is rounded off using round() method\n",
    "        percentage = (y * 100)/line_count\n",
    "        \n",
    "        # printing the data on interactive ipython\n",
    "        print(soc_jobtitle[x],y,percentage)\n",
    "        \n",
    "        # preparing string to insert into the .txt file\n",
    "        line = str(soc_jobtitle[x]) +\";\" +str(y)+ \";\" + str(round(percentage))+\"%\" +\"\\n\"\n",
    "        myoccupations.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 Certified states of year 2014: \n",
      "\n",
      "\n",
      "CA 76670 16.845218216652313\n",
      "TX 65659 14.425983864447296\n",
      "NJ 61088 13.421686323449283\n",
      "NY 33471 7.353936336631923\n",
      "IL 25706 5.647882867839629\n",
      "PA 21229 4.664238131228798\n",
      "MD 20197 4.4374967043397255\n",
      "MA 15666 3.441987590740513\n",
      "VA 15595 3.4263881321076406\n",
      "MI 15352 3.372998435659923\n"
     ]
    }
   ],
   "source": [
    "print(\"Top-10 Certified states of year 2014: \\n\\n\")\n",
    "with open(\"../output/Top_10_states.txt\",'w') as mystates:\n",
    "    \n",
    "    for x,y in final_states:\n",
    "        percentage = (y * 100)/line_count\n",
    "        print(x,y,percentage)\n",
    "        line = str(x) +\";\" +str(y)+ \";\" + str(round(percentage))+\"%\" +\"\\n\"\n",
    "        mystates.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
